{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import geatpy as ea\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the diagonal element of the bilinear matrix to be always zero\n",
    "\n",
    "class BilinearModified(nn.Module):\n",
    "    __constants__ = ['in1_features', 'in2_features', 'out_features']\n",
    "    in1_features: int\n",
    "    in2_features: int\n",
    "    out_features: int\n",
    "    weight: torch.Tensor\n",
    "\n",
    "    def __init__(self, in1_features: int, in2_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in1_features = in1_features\n",
    "        self.in2_features = in2_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in1_features, in2_features), **factory_kwargs))\n",
    "\n",
    "        if bias:\n",
    "            # Use register_buffer to make bias a non-trainable fixed value (-70)\n",
    "            self.register_buffer('bias', torch.tensor(-70.0, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        # if bias:\n",
    "        #     self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        # else:\n",
    "        #     self.register_parameter('bias', None)\n",
    "        # self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        bound = 1 / math.sqrt(self.weight.size(1))\n",
    "        nn.init.uniform_(self.weight, -bound, bound)\n",
    "        # if self.bias is not None:\n",
    "        #     nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "        # Zero out the diagonal elements of the weight matrix\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(self.in1_features, self.in2_features)):\n",
    "                self.weight[:, i, i] = 0\n",
    "\n",
    "    def forward(self, input1: torch.Tensor, input2: torch.Tensor) -> torch.Tensor:\n",
    "        # Ensure diagonal elements are zero during the forward pass as well\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(self.in1_features, self.in2_features)):\n",
    "                self.weight[:, i, i] = 0\n",
    "                \n",
    "        return F.bilinear(input1, input2, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return (f'in1_features={self.in1_features}, in2_features={self.in2_features}, '\n",
    "                f'out_features={self.out_features}, bias={self.bias is not None}')\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "# input size: (batch_size, N_synapse, time_dur)\n",
    "# kernels size: (N_synapse, 1, time_dur)\n",
    "# output after convolution: (batch_size, N_synapse, time_dur)\n",
    "# transpose to (batch_size, time_dur, N_synapse)\n",
    "# bilinear matrix size: (N_synapse, N_synapse, 1)\n",
    "# output size: (batch_size, time_dur)\n",
    "\n",
    "class DBNN(nn.Module):\n",
    "    def __init__(self, num_dimensions, time_dur, device):\n",
    "        super(DBNN, self).__init__()\n",
    "        self.num_dimensions = num_dimensions\n",
    "        self.time_dur = time_dur\n",
    "        self.device = device\n",
    "        # intinial values\n",
    "        self.tau_rise = nn.Parameter(torch.ones(num_dimensions).to(self.device) * 30)\n",
    "        self.tau_decay = nn.Parameter(torch.ones(num_dimensions).to(self.device) * 20)\n",
    "        self.omega = nn.Parameter(torch.ones(num_dimensions).to(self.device) * 2)\n",
    "\n",
    "        self.bilinear = BilinearModified(num_dimensions, num_dimensions, 1).to(self.device)\n",
    "\n",
    "    def create_kernels(self):\n",
    "        T = torch.arange(self.time_dur).to(self.device)\n",
    "        N = self.num_dimensions\n",
    "        net_tau_rise = self.tau_rise.unsqueeze(1)  # (N, 1)\n",
    "        net_tau_decay = self.tau_decay.unsqueeze(1)  # (N, 1)\n",
    "        net_omega = self.omega.unsqueeze(1)  # (N, 1)\n",
    "\n",
    "        kernels = net_omega * (1 - torch.exp(-T / net_tau_rise)) * torch.exp(-T / net_tau_decay)\n",
    "        return kernels.unsqueeze(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        kernels = self.create_kernels()\n",
    "        kernel_flipped = torch.flip(kernels, dims=[2])\n",
    "\n",
    "        # Convolve using the kernel (perform manual convolution)\n",
    "        y = torch.nn.functional.conv1d(x, kernel_flipped, groups=self.num_dimensions, padding=self.time_dur - 1)[:, :, :self.time_dur][:,:,:self.time_dur]\n",
    "        y_permuted = y.permute(0, 2, 1)\n",
    "        bilinear_term = self.bilinear(y_permuted, y_permuted)\n",
    "        linear_term = torch.sum(y_permuted, dim=2).unsqueeze(-1)\n",
    "        output = bilinear_term + linear_term\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSP_kappa_data = np.load('/data/mjy/Yeqiang/sub_threshold_single_kappa.npz')\n",
    "singlev = PSP_kappa_data['singlev'][:, ::10]\n",
    "kappa = PSP_kappa_data['kappa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/data/mjy/Yeqiang/subthres_coarse_data.npz')\n",
    "\n",
    "input_data = data['coarse_ip_m_array']\n",
    "output_data = data['coarse_v_array']\n",
    "\n",
    "train_input = torch.tensor(input_data[:1000], dtype=torch.float32).to(device)\n",
    "train_output = torch.tensor(output_data[:1000], dtype=torch.float32).to(device)\n",
    "test_input = torch.tensor(input_data[1000:], dtype=torch.float32).to(device)\n",
    "test_output = torch.tensor(output_data[1000:], dtype=torch.float32).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bilinear_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
